# Demystify AI: Generative AI

1.
For people who already know about AI, could be a refresher.<br>
But people who starting this journey first time, probably a little bit more exciting.<br> 
So starting with my introductions, I'm ....<br><br>

2.
## Demystify AI: What's real, What's next, What matters

I'll run through the presentation and we'll leave some time for the question and answer.<br>
I would love to make it interactive, so feel free to jump in for questions.<br> 
All right, so demystify what's really AI is, what's next? What matters?<br>
But I wanted to throw some numbers.<br> 
That's how I would always like to do in my presentation that I always start working backward.<br>
In terms of what is the potential of the technology we're talking about, so I'll throw some numbers here and let me know where where you heard these numbers.<br><br>

3.
### AI landscape in 2030
### AI Retail Market is estimated to be ¬£28 B.
### 90% of customer decisions will be made with AI.
### 33% of Enterprise Software Applications will use Agentic AI. (<1% right now)>)

<br>

So first one is **AI retail market is estimated to be 28 billion**.<br> 
So if you don't start investing in possibilities of implementing AI in some of those core business areas, probably we're losing bit chump there.<br>

Then in terms of the **decisions will be made with AI**, a huge number, 90%.<br> 
But we started our journey back in 2013.<br> 
I think it's something I'll take you through with the curtain razor style, but I think we only started thinking about AI about a decade ago,<br> 
We didn't make a mark, until 2022 where everything is disrupted with the **rise of the transformer based technologies** and those **large language models** and the **foundation models**.<br>
So now we see every other day of the week, new foundation models.<br>
New language models are coming into the market disrupting the industry.<br> 

And also **33% of the enterprise software application will use Agentic AI**.<br> 
So this is another next level of conversation.<br> 
So we have AI now agentic AI performing different task and most of those enterprise application. We expected to have agent AI by 2030.<br>
Right now it is less than 1%, so that's tells us that how disruptive this particular technology is in the industry.<br><br>

4.
```
AI (Artificial Intelligence)
‚îî‚îÄ‚îÄ ML (Machine Learning)
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Supervised Learning
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Unsupervised Learning
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Reinforcement Learning
    ‚îî‚îÄDL (Deep Learning)
      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Convolutional Neural Networks (CNN)
      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Recurrent Neural Networks (RNN)
      ‚îî‚îÄ GenAI
          ‚îú‚îÄ LLM
          ‚îú‚îÄ NLP
          ‚îú‚îÄ RAG
          ‚îú‚îÄ Text
          ‚îú‚îÄ Images
          ‚îú‚îÄ Audio
          ‚îî‚îÄ Video
```

So what is AI.<br>
As you can see it describes nicely in terms of where AI sits.<br>
so AI is basically the whole **umbrella**.<br> 
Then we have traditional machine learning, neural networks, deep learning and then today what I'm going to talk about is a **generative AI**.<br>
I just wanted to make it clear that I think we're not going to cover the whole AI's piece.<br>
But specifically about what possibilities are there for the generative AI.<br>
It's a big, big, big space.<br><br>

5.
## What is Generative AI?
### Content Creation
### Diverse Output:
- Text
- Images
- Audio
- Video

<br>
So I'm going to just quickly ask this question to the audience.<br> 
Obviously, don't sweat on it, but if you have something comes in your mind, any definition of generative AI, feel free to put into a chat.<br>
Obviously I'll give the answer here, but quickly 30 seconds if you can put a word or anything.<br><br>

But in the real definition of generative what we say it's a **create**, it basically generates **new content**, right.<br> 
And that content includes **text, images, audio, video**.<br>

So if you look at the traditional ML models and when we actually train those models, it actually give you the predicted answer.<br> 
So it's that doesn't generate new concepts, it generate at least the the the deterministic answers like you know weather.<br>
Now how much sales are going to grow in next 20 years, next two years or three years, right.<br> 
But what is different with from Amazon generative AI is it generates a novel contents which includes **text, images, audio and video**.<br> 
I think this is where we found the big potential in generative AI.<br>
So on the image on your right is also generated by General AI, so this is using text to image for example.<br> 
So as you can see the the level of accuracy as you can see and the text.<br>
So it's pretty fascinating.<br><br>

6.
# AI & Language: A Journey of Understanding
### From Vector Space to Conversational Intelligence
<br>

So next one, what I'm going to do is obviously we know what is **generative AI**.<br>
Now, it generates really creative in terms of thinking and it generates novel content.<br> 
It just doesn't just analyze but it also generates **new content**.<br> 
But where do we, where did we start? So I just wanted do.<br>
Rewind a little bit in the in the past and and I'll give you the the **walk through in terms of where we started**.<br><br>

7.
# AI & Language: A Journey of Understanding

### 1.Word2Vec (2013)
Words as Vectors, Finding Relationships
### From Vector Space to Conversational Intelligence
<br>

So in **2013**, I think probably you know when you're had a very basic terms when you type things you basically predict used to predict next word.<br>
Like I want to go to cinema, right? So it used to predict the next word.<br> 
So that comes from this algorithm called **WORD2VEC**.<br> 
But there are basically the way we designed this word to work is we basically took all the words from the **English dictionary** and we started assigning then numbers what we call **Vectors**.<br> 
and based on their **semantic similarity**, we actually start putting them into **two-dimensional array**.<br><br>


8.<br>
# Vector space
```
‚îÇ
‚îÇ        * Queen = [0.3, 0.9]
‚îÇ
‚îÇ             * King = [0.5, 0.7]
‚îÇ
‚îÇ
‚îÇ              * Woman = [0.3, 0.4]
‚îÇ
‚îÇ                   * Man = [0.5, 0.2]
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```
### king - man + woman = queen

<br>

So as you can see **King** and **Queen** and **man** and **woman**.<br> 
Basically in this two or three-dimensional array sits together because they are semantically similar and we used to assign those **Words numbers**.<br>

For example, Queen will have certain dimension, One man will have certain dimensions.<br> 
So using this vectors or numbers we started calculating the the statistical relationship between those numbers right to add predict the next word.<br>
And when I ask this algorithm, what does can you give me the Queen? So it used to do a mathematical calculation, <br>
for example **king minus man plus woman is equal to queen** because now we took the words we assigned the vectors or numbers to each word.<br>
And now we started calculating to arrive at a specific word.<br> 
So that's what we used to back in 2013.<br> 
That's where the actual journey started for the **language models** itself.<br><br>


9.
# AI & Language: A Journey of Understanding

### From Vector Space to Conversational Intelligence
### 1. Word2Vec (2013)
Words as Vectors, Finding Relationships
<br>

### ##################################### LAZY ~~PERSON~~
<br>

Then let's put this language model to test.<br>
So I gave this language model a full sentence.<br> 
There is a full sentence, but since this language model only have visibility or a context of **single word** which is the previous word it tried to predict that word.<br>
And it says person.<br> 
**Anyone have any thoughts** in terms of what that next word could be if people have heard about this sentence which have high hidden right now, probably they know the answer.<br> 
**But anyone?** So what could brings your mind when you say lazy so?
What could be the next predicted word for that? 
Anyone want to try before we move on? 
So I tried person tired dog.<br>
So I think I wouldn't reveal the answer yet, but person is absolutely wrong.<br> 

So this sentence which is hidden of this particular word at the end is wrong because I said this particular algorithm only have context of the **previous word**, it doesn't have the context of the whole sentence.<br>
<br>


10.
# AI & Language: A Journey of Understanding

### From Vector Space to Conversational Intelligence
### 1. Word2Vec (2013)
Words as Vectors, Finding Relationships
### 2. RNNs / Seq2Seq (2014 - 2016)
Sequences and Context, Early Translation

<br>

### ################### FOX JUMPED OVER THE LAZY ~~CAT~~
<br>

Then we moved on in **2014-16**.<br>
We also had algorithms called **recurring neural network**.<br>
So these are quite sophisticated than word to work where it could store actually process this word sequentially<br> 
and it could store bit more context than what word to work used to store for example.<br> 
Now it has more context.<br>
Like **Fox jumps over the lazy**, this particular algorithm has some context.<br> 
It's there is a animal involved in this sentence, Fox.<br> 
So basically the next word could be an animal, so it tries to predict that word like **cat**, right? But.<br>
That cat is the wrong in the context of that particular sentence because it tried it made a best attempt to predict the next word based on the limited context it has.<br> 
It still failed, so these algorithms still failed in those things because **it doesn't store full context.** <br>
<br>


11.
# AI & Language: A Journey of Understanding

### From Vector Space to Conversational Intelligence
### 1. Word2Vec (2013)
Words as Vectors, Finding Relationships
### 2. RNNs / Seq2Seq (2014 - 2016)
Sequences and Context, Early Translation
### 3. Transformers / Attention (2016 - 2017)
Focusing on importance, Parallel Processing

<br>

Then we had something called **Transformers**, which is the **breakthrough** in the **language models in 2017**.<br>

<br>


12.
# Rise of Transformer Architecture - 2017
## Attention Is All You Need
White Paper

### Attention Only! (No more word-by-word)
### Self-Attention
### Many "Heads" of Attention (Multi-Head Attention)
### Super Fast Training

<br>

So the Google brain, some of those Google brains come together and created this paper called **attention all you need**.<br> 
So that basically define the direction of the large language models, what we see and their capability.<br>
and their disruption so what this paper introduced.<br>
I'm not going to go through the whole paper, but some of those key things is **attention** only.<br> 
So it's not this **transformer** based technology or algorithm.<br> 
What it does is the way we analyze the language, it doesn't analyze word by word, just the way we RNNS and the word.<br>
Algorithm used to do it basically takes the whole sentence in the **context**, then **self attention**.<br> 
So while analysing that sentence it also knows which word appears well and also how those words are related to each other.<br> 
So it has it is more intelligent than those two previous two algorithm.<br>
And it also has heads, many heads like, just like there are multiple human brains thinking in different directions in parallel.<br> 
and try to understand the complex relationship between those sentences and the words in the sentence.<br> 
So there's more intelligent.<br> 
And also knowing where words are, because this word to where can RNNS.<br> 
The previous algorithms right? They just operated on the word by word.<br>
They know where these words appear in the sentence, but since these **transform waste algorithms**, they actually take the whole sentence they needed to understand and number or tag where each those words appears in those sentences to be able to make the right judgement.<br> 
And then if they're super fast tuning, I mean, since they're super fast as you can see those large language you probably heard **18 billion parameter**.<br> 
So because they are super fast in training, we can actually pump in quite a lot of data into them into them while training and because they can process the data in **parallel**.<br>
A good job, right?

<br>


13.
# AI & Language: A Journey of Understanding

### From Vector Space to Conversational Intelligence
### 1. Word2Vec (2013)
Words as Vectors, Finding Relationships
### 2. RNNs / Seq2Seq (2014 - 2016)
Sequences and Context, Early Translation
### 3. Transformers / Attention (2016 - 2017)
Focusing on importance, Parallel Processing
### 4. BERT / Large LLMs (2018 - 2020)
Deep Context, Pre-training, Many Tasks

<br><br>

So the actual right answer is the **dog**.<br> 
I'm not sure many of you have used this sentence before, but this is a famous English panogram which is being used for the typing where you have all the letters in your alphabets.<br> 
This model probably must have trained on the large data and must have seen this particular sentence, and that's why it could actually predict the next word correctly.<br> 
And if you give this word it this sentence with dropping any a word in between.<br>
And ask these large language models to predict it can actually predict any word in any sequence right in this particular sentence, right.<br> 
Even if you type **fox or brown fox**, it will give you the full sentence.<br> 
Try that probably sometimes in your ChatGPT.<br>
So I think that's the journey of the large language models, right?<br> 
So but we also have models which are text to image, right? <br>
So if you type a prompt and it actually generates a nice **text**, but let's try to understand how they work behind the scene, right.<br>
<br>


14.

# Little about - Text to Image Models

## Training Process
#### ----------------------------> Diffusion process
### Data --------- > CAT IMAGE BEING DISTORTED -------> Add Noise 
#### <----------------------- Gerarative reverse denoising process

### Prompt: Generate an image of Cat driving a Tesla S model. The cat has white and brown fur.

<br><br>


So these are **text to image models**.<br> 
So what we do, we actually give a lot of **training images**.<br>
So these text to language models, and while training them, we actually add the **noise**, which means add the distortion to the images and every time we add the distortion to the images we ask model to predict again whether that was the original image or not.<br> 
So we keep doing that until the image is fully noised or fully.<br>
And if the language, the model, the foundation model is able to predict that image, then we say that that's the **accuracy** we wanted.<br> 
So that's how we train those image models and then we give the from those image models<br> 
for example, generate an image of a cat driving the **Tesla S model**, the cat has, right.<br>
And **Brown**.<br> 
So this is the prompt.<br> 
If I gave that, it converts that from into an instruction and it goes through all the images it trained on and it generates a nice image.<br> 
It must have trained on some of those large models and the different guides.<br> 
So it picks up.<br>
Those but different images and put this image together so you can see this is real life amazing image which is created by **generative AI**.<br> 
This is I've used a **vertex AI image chain model** for this particular one.<br> 
So this is how the **image models** train.<br>
<br>


15.

# Explosive and disruptive adoption

## Time to 1 Million Users

| Online Platform (Year) | Graph (Linear Scale) | Time Taken to Reach One Million Users |
| :--------------------- | :------------------- | :---------------------------------- |
| Threads (2023) | $\text{I}$ | 1 hour |
| **ChatGPT** (2022)         | $\text{I}$ | **5 days**                              |
| Instagram (2010)       | $\text{I}\text{I}$ | 2.5 months                          |
| Spotify (2008)         | $\text{I}\text{I}\text{I}\text{I}$ | 5 months                            |
| Dropbox (2008)         | $\text{I}\text{I}\text{I}\text{I}\text{I}$ | 7 months                            |
| Facebook (2004)        | $\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}$ | 10 months                           |
| Foursquare (2009)      | $\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}$ | 13 months                           |
| X (Formerly Twitter) (2006) | $\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}$ | 2 years                         |
| Airbnb (2008)          | $\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}$ | 2.5 years                           |
| Kickstarter (2009)     | $\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}$ | 2.5 years                           |
| Netflix (1999)         | $\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}\text{I}$ | 3.5 years                           |


<br><br>

Moving on with the excitement, I just wanted to throw this numbers because now we've seen the potential and the possibility of the AI.<br> 
So this is a **no brainer**, so these are the basically different **social media**.<br> 
You know, the hits.<br>
I think for last 20 years or something as you can see, the **Netflix Dropbox**, those are big numbers, <br>
but when **ChatGPT** introduced in **2022**, can you guess how long did it take to become a **million user application**? Any guesses?<br>
Yeah, it only took **five days** on ChatGPT to become a million user application, so that's phenomenal, right? <br>
So that actually tells in terms of the kind of excitement and kind of things it can do for an, I think we.<br>

Ever since chatGPT, and obviously other tools came in, I started planning all my holidays and everything using those things.<br> 
I think it's really easy and it's so accurate.<br> 
I don't have to do any things else.<br>

<br><br>


16.

## üî¨ Shift in AI: From Traditional ML to Generative AI

| Traditional Machine Learning (ML) | Generative AI (Gen AI) |
| :--- | :--- |
| **One Model, One Task** | **One Foundation Model, Multiple Tasks** |
| * $\to$ ML Model A $\to$ **Recommendation** | * $\to$ **Recommendation** |
| * $\to$ ML Model B $\to$ **Search** | * $\to$ **Search** |
| * $\to$ ML Model C $\to$ **Chatbot** | * $\to$ **Chatbot** |
| | * $\to$ **Content Generation** |
| | * $\to$ **Image Generation** |
| | * $\to$ **Code Generation** |

---

<br><br>

And there's another reason, obviously, apart from helping solving your day-to-day problems, why they become sensation in the industry because traditional ML, we can probably we know.<br> 

I think since we are AI first company we all know that there's so many models we need to build for so many use cases.<br> 
If it didn't dumb down or external use cases.<br> 
For example fraud detection, recommendation, search, chat bot.<br> 
So for every specific use cases we end up building new machine learning models.<br> 

But because the way large language model works.<br>
You only need one foundation or one large language models to uncover all those use cases.<br> 
So imagine ease of maintaining those different machine learning models, the overhead and accuracy and the bias and the monitoring and all this stuff.<br> 

So that's literally reduced significantly with introduction of large language models, but just making this statement carefully here because there are still some use cases which needs the deterministic answer, we still need machine learning.<br> 
So some of those use cases what we do is we use machine learning.<br>

To get the input, get the output and then we pass this output to the foundation models to predict or create another content right?<br> 
So machine learning is not going anywhere.<br> 
The machine learning is complementing the the generative AI output.<br> 
So, but there are some outputs like recommendation, search, chatbot.<br>
Those can be built using those next Gen foundation or large language models.<br> 
That includes, as I said, image generation, code generation.<br> 
We know that we are using quite a lot copilot these days to generate the code, code reviews, content generation.<br>

And all the stuff.<br> 
So we'll talk about, you know, why while this technology is quite exciting and creative disruption in the industry, but there were a lot of things.<br> 
What are the challenges we are facing.<br> 
So we'll probably come to that at the end once we get through this excitement, what is the possible?

<br><br>


17.
## üèóÔ∏è Four Generative AI Technology Stack Layers

| | | |
| :--- | :--- | :--- |
| **4. Apps** | **ChatGPT, Copilot** | **Software applications** that primarily use GenAI models to perform a specific task. This includes both industry- and function-focused applications. |
| **3. Engineering Tools** | **Vertex AI** | **Tools** that enable enterprises to **operationalize** GenAI models. This includes model-centric and data-centric tools for deployment and management. |
| **2. Models** | **OpenAI, Anthropic** | Includes both core **GenAI foundation models** (like Large Language Models - LLMs) and **domain-specific models** that are tailored to a given industry or use case. |
| **1. Infrastructure** | **AWS, Google Cloud** | **Infrastructure components** that form the base layer, used to build out Generative AI applications, such as compute, network, and storage. |

---

<br><br>

Moving on, all this to work together right, there's ecosystem needs to be in place.<br> 
Any disruptive technology, a lot of things we need to put in place, they need to work together, hand in hand to make it work.<br> 
So similarly for generative area, we have a lot of the technologies that has to come together.<br>

Starting with infrastructure we need solid compute powers.<br> 
We also need that the chips like NVdi, MD and all those chip making companies and amanapurna from AWS.<br> 
They're making those chips for us, you know to train those large language models, right.<br> 
And also the compute GPU enabled compute so that we can train those models faster.<br><br><br>


And then the the companies were actually building those foundation models, for example openAI and entropic.<br> 
So they are building new models, image text to image, image to image.<br> Now they are building like we used to have that concept called text to image, text to video, text to audio.<br>

You know now that concept is slowly fading because all the models we are building at multi model, right? So they can accept input images, videos, text and they're also putting images, text, videos and audios right? So they are the concept of those different models are fading away.<br> Now there's one foundation model incorporates in everything.<br> 

So these are the model making companies like openAI, anthropic.<br> 
There are also other people open source models, right?<br> 
There are two types of models that's open source and subscription based.<br> 
The companies who are actually building the models and putting into the market to as a subscription model.<br> 
And there are companies who are actually or the communities are building the models and operating as a free source, right? <br>
So these are models available.<br><br><br>


But these models we won't be able to use it unless we have engineering tools.<br> 
So there are companies like Vertex and Amazon Bedrock.<br> So who actually brings all those models commodities

"subscription model.<br> And there are companies who are actually.<br>
Or the communities are building the models and operating as a free source, right? So these are models available.<br> But these models we won't be able to use it unless we have engineering tools.<br> So there are companies like goole **VertexAI** and Amazon **Bedrock**, **Azure AI**.<br> 
So who actually brings all those models commoditize those models into a platform so that we can.<br>
Use those platforms to build our applications.<br><br><br> 



So now we are familiar with **ChatGPT**, Co pilot and then you know different type of conversation chat boards.<br> 
Those are nothing but actually application built using these engineering tools and the whole obviously the whole Technology stack, <br>
So just to be mindful that no generative AI won't work on it.<br> 
So like once we build the model, it's not there immediately to use, but the whole ecosystem has to come together to put an application or an industry great solution together.<br>
Make sense?

<br><br>


18.
## Large Language Models (LLMs) & Foundation Models (FMs)

```text
CORE_FOUNDATION_MODEL (FM)
‚îú‚îÄ‚îÄ TEXT_OUTPUTS (Text-to-Text LLMs)
‚îÇ   ‚îú‚îÄ‚îÄ Summarization
‚îÇ   ‚îî‚îÄ‚îÄ Translation
‚îÇ
‚îú‚îÄ‚îÄ IMAGE_OUTPUTS (Text-to-Image FMs)
‚îÇ   ‚îú‚îÄ‚îÄ Image_Generation
‚îÇ   ‚îú‚îÄ‚îÄ Art_Generation
‚îÇ   ‚îî‚îÄ‚îÄ Design_Prototyping
‚îÇ
‚îú‚îÄ‚îÄ AUDIO_OUTPUTS (Text-to-Audio FMs)
‚îÇ   ‚îú‚îÄ‚îÄ Speech_Synthesis
‚îÇ   ‚îú‚îÄ‚îÄ Music_Generation
‚îÇ   ‚îî‚îÄ‚îÄ Sound_Effect_Generation
‚îÇ
‚îú‚îÄ‚îÄ VIDEO_&_COMPLEX_OUTPUTS (Text-to-Video FMs)
‚îÇ   ‚îú‚îÄ‚îÄ Music (Contextual)
‚îÇ   ‚îú‚îÄ‚îÄ Form (Processing)
‚îÇ   ‚îú‚îÄ‚îÄ Deen-Claw (Specific Term)
‚îÇ   ‚îú‚îÄ‚îÄ Audio_Output (from video context)
‚îÇ   ‚îú‚îÄ‚îÄ Question_Answering
‚îÇ   ‚îî‚îÄ‚îÄ Creation / Generation
‚îÇ
‚îî‚îÄ‚îÄ ACTIONABLE_USE_CASES
    ‚îú‚îÄ‚îÄ Chatbots / Virtual_Assistants
    ‚îú‚îÄ‚îÄ Code_Generation
    ‚îú‚îÄ‚îÄ Content_Creation
    ‚îî‚îÄ‚îÄ Blog_Post_Generation
```

<br>
And this is basically now we talked about how we build those **generative AI model**, right? <br>
What are the use cases we uncover? <br>
But I assume that it's a foundation of large language models.<br> So as you can see it actually uncovers or helps us to deliver different use cases like **summarization**.<br> So if you have a large document you want to summarize or create a summary, we have a number of emails you want to create a summary of that e-mail you want to translate from one language to other.<br>
Then you have you want to create a image, for example you want to build a **creative builder** like, you know creative ads, right? You just support give the text on the the brand logos and everything and it has ability to create and add the different dimensions.<br> Then also you have a **two-dimensional image** you want to convert it into three.<br>
If you want to create a **song**, obviously I'm not a musician, but now I can create my songs and and I can actually train the model on my own voice and create my own album, right? It wasn't possible before, but of these today's **generation models** now it allow us to do that.<br>
And then some of the use cases like **code generation**, we talked about blog post like I think we quite a lot of are quite afraid of writing these days.<br> But if you have a very small summary of something you want to write, you can actually ask those models to create a **blog post** and put it and share it with the team, right? It's no more a restriction or a co-ordinator.<br>
So these are some of the use cases which allows even the video right **text to video**.<br> I think, I'm not sure whether you've heard, but there are a lot of videos floating around on YouTube you it's hard to differentiate whether those are actually created or short actually or it's created by Jemmy, right?
So a lot of **potential**.<br> It's obviously it also presents a lot of **risk** as well, but.<br>
The potential is high and and obviously those **risks can be mitigated with a lot of quadraills**.<br>
right.<br> So these are some of those use cases, this not limited, but I just wanted to bring into your notice and now the next session would be I think the next section would be I'm going to play series of videos.<br>
Which talks about how actual real production use cases are solved by the **Gen AI**.<br> So those goes through different like images, **business intelligence**, the different types of use cases, **customer service** and all the stuff.<br> So I think it hopefully will give you some idea.<br>
In terms of **Virginia** can actually help us.<br>

19.<br>
So but before that, sorry, I just jumped too much.<br> So before that, I want to introduce you with some of those key concept of **generative AI**.<br> So starting with **prompt engineering**, obviously those large language models, right, they won't work on its own.<br> So they have we have to give them instructions and the right instructions.<br>
Every large language models obviously has separate, different levels of capabilities.<br> A model will probably help you summarization task.<br> The other model probably help you with **Q&A**.<br> The other model probably help you with the search things right.<br> So every model had its own set of capabilities.<br>

And how we actually leverage or exploit that capability is by giving the right instructions, so the prompt is nothing but the combination of your **context**.<br> What context that model needs to perform on, give the clear instruction, and then also some of the **examples**.<br> Right.<br> So what we call when we talk about prompt engineering.<br>
Techniques we have a **0 shot**, which means you don't give any any examples to the to the large language model and our language model uses own knowledge.<br> But if you want to be very specific you give **few short examples**.<br> This is how it's you're suggested to do.<br>
Right.<br> Some of the examples could be, you know, I want to go on holiday to Rome.<br> Can you suggest the destination and flights? Right.<br> So it's very basic problem but if you have gone down, oh, I want to visit some monumental places I want to, you know, live somewhere or have a nice restaurants and pizza places.<br>

So you give a lot of context and instructions, then model probably do a better job at it rather than going back and forth with the model.<br> So for the product generating one thing we need to make sure we give enough context, we give enough instructions, right, you know, clear instructions and then also some of the examples to make it really.<br>
For your expected output, right? And there are techniques from **engineering techniques**.<br> I think we have a session coming in.<br> I think today or tomorrow about the prompt engineering.<br> I guess I think I'll just give you know the we have a **zero shot prompting** as I said the **few shot prompting** which means you give more examples.<br>
Then if you have **chain of thought**, so before obviously just like human being before doing something you think through your steps before you actually do take some actions, what we call **chain of thought**.<br> So it basically is goes and analyze the instructions and the context you have given.<br> It actually says oh if I do this I do this.<br> So it basically builds the **chain of thoughts**.<br>
Before he actually gives you a response back so you can actually, you know, compare models and think before you say, think before this.<br> So you can actually add those kind of **instructions to your model**.<br> So it can think through today's models.<br> I think it comes with built in functionality we call **reasoning**.<br>
So you have to give really less amount of prompt these days because we improved over the period of time.<br> The next one is **rag retrieval augmented generation**.<br> I think why the rag is important because when we train these large language models, you know these language models are trained on large corpus of the data.<br>

Possibly we can have and when we release the models the models are restricted to that cut off date only.<br> So if the model is released, let's say to three months ago that models knowledge is only restricted and it doesn't have the knowledge of your own **enterprise data**, right?
So if you want to one model to give you relevant responses, accurate responses, we need to augment this models information or models, data or models **knowledge with our own knowledge**.<br> So that's what we call **rag**.<br> So while actually answering or while actually asking models to do certain.<br>

The task we said use your knowledge but use these knowledge as well, which is my own **enterprise knowledge**.<br> It's called **rag**, so we'd actually give we augment models knowledge with our own enterprise knowledge.<br> So that model will actually use our relevant information and give you the right response.<br>

Because these models won't have the context of your own data, so that's the call that's called **rag**, and most of the applications and solutions which we've built today as using generative where you use **rag methodology** then we have **fine tuning**.<br> So all this all these models pre built or pre trained on.<br>

knowledge with our own knowledge.<br> So that's what we call rack.<br> So while actually answering or while actually asking models to do certain.<br>
The task we said use your knowledge but use these knowledge as well, which is my own enterprise knowledge.<br> It's called rag, so we'd actually give we
augment models knowledge with our own enterprise knowledge.<br> So this model will actually use our relevant information and give you the right response.<br>
Because these models are we have to continue of from the data, so that's the call that's called rag, and most of the applications and solutions which
we've built today is using generative there you use rag methodology then we have fine tuning.<br> So all this all these models pre built or pre trained on
large amount of data it's they're not designed to do specific tasks.<br>
So sometimes you know these models are general purpose models.<br> You can do everything and anything, but if you rank let's say for example, you want to
create a model specifically for your industry, like for retail media, you want to build model, we can predict the consumer behaviour.<br> So whatever it
is, right?
So this model must have trained on some consumer behaviour, some data in the past, but they're not actually that good.<br> So what we need to do is we
need to fine tune tune these models with our own data sets, so that actually will to use its own power, but also becoming more relevant to audio cases.<br> So
it's called fine tuning so.<br>
we don't actually retrain these full models.<br> But we only need small chunk of data to fine tune them to perform a specific task.<br> So that's the fine
tuning bit.<br> And then alerts, obviously we human build these models, right? So we are not going away.<br> I think we.<br>
Always debate on that once AI you know take over the world, but it's not going to happen soon.<br> So that's why to make these models better and and
smarter over the period of time, we need to continuously provide them feedback, right? So whether you did this job right now, you did do this job,
right? So we provide this feedback continuously to those models so that they.<br>
Get better over the period of time, right? So that's why we have reinforcement learning from human feedback.<br> So every time you build solutions, build
applications using these models, what we include humans in the some of the response from those models, we ask human to review and rank.<br>
and we use that information to reinforce in reinforcement learning and make those models better for all use cases.<br>
All right.<br>

20.<br>
All right, so last bit before we move on to the exciting videos, this is how actually the the prompt, the actual model works behind the scenes.<br> So what
happens when we actually ask the question to ChatGPT or any model, right for for pilot dig or copilot, what happens behind the ocean?
So you basically give a prompt and all those prompts are converted into tokens because machine doesn't understand text.<br> We have to convert those into
a, a binary or whatever tokens format a number so that machine can actually use for the computation, right? So what we call tokenization?
So when we give sentence or a question, those questions are broken into into tokens.<br> Those are nothing but word.<br> So three fourth of word is called
token.<br> In large language model, it doesn't give a whole word.<br> It's only three fourth of the word is considered as token.<br>
And those tokens passed into the processing unit we call transformer we talked about before and this transformer basically goes through series of
operation computations, right? So this is the question.<br> Now what I need to do.<br> So it is actually after processing it generates the the output.<br>
We predict the next token.<br> So I think it's something I wanted to also clarify that when we actually going through those large language model, the way
they generate output, they generate output one word at a time.<br> It's not sure early adopters of ChatGPT where it came.<br> You can see when you actually
Ask any question to ChatGPT.<br> It used to spit one word at a time right.<br> Nowadays I think if you just take the whole get the whole human the whole
response, but previously it used to predict one word at a time, so that's how the large language models are operating behind the scenes.<br> So it
actually predicts one word or one token at a time.<br> So when as soon as it predicts token,
Obviously it has to go through the detokenization because you won't understand the the numbers, so a detokenizer is converted into a text and the
response is finally sent back to you.<br> So this is what you know needs to happen behind the scene before you actually get those you know.<br>

actually predicts one word or one token at a time.<br> So when zoom as it predicts 1 token.<br>
Obviously it has to go through the **detokenization** because you won't understand the the numbers, so a **detokenizer** is converted into a text and the
response is finally sent back to you.<br> So this is what you know needs to happen behind the scenes before you actually get those you know.<br>
answers and obviously get pulled over that.<br> How can Al can answer those questions? So this is what needs to happen **behind the scene**
All right.<br> So as you can see little example, this is how it predicts, although it has the whole context, but it always predicts the next token or one
word at a time.<br>
All right, last bit before prise before the videos, we **OK**, so we create the disruption with generating a Large Language models but we didn't
stop there right? How we can use this capability to to create an **autonomous solutions** like **robots** and and the **agents** right.<br>

**21**

So the the clear definition of the simple definition of our agents are **basically** you give them task and they plan this task.<br> Then they execute the
task and they also reflect on those tasks**.<br> If they did a good job, bad job it basically you know it understand itself, understand.<br>
Itself in terms of whether I've done the good job or could have done the better job right? So it basically do goes through a continuous we call
reinforcement learning right continuous feedback loop to get better over the **period of time** and time job.<br> So for example this agent is a booking agent.<br>
So I'm just giving a goal like a plan, a trip to Rome and then now it starts planning, right? So if I want to plan a trip, I need to book a flight,
choose dates, book flight and create itinerary and then **create an then action upon it.<br>** So it's basically agent will go and
book a flight, book, a hotel and everything, right.<br> Find the **places**, find and book a ticket for those attraction.<br> All the stuff.<br> So it does all of
those things and then basically at the end of this **execution**, if it says, **oh, did I book the best hotel**, whether the best hotel was available
and I missed it.<br> So it does that, it does that **introspection**.<br>
And it learns itself so next time when you ask book a **room**, probably your trip could be cheaper or could be better than the before, right? So that's
how agent is designed.<br> And obviously we'll see **agent everywhere**.<br> And you'll also see in the, you know, the tta statistic, the beginning of the session
is said that.<br>
You know, **33% of the application**, 30% of the **enterprise application will be with Agent TKI by 20-30**.<br> So that's why because these **agents** are quite
**smart**.<br>
All right, so to go, moving on to the demo sessions or I would say the **use case sessions which are real life**, I'll start with my favourite
one meet new **Alexa**.<br> Obviously we also have similar **Gemini, but this one I like**.<br> This is basically an example of **text to speech**.<br> I'm going to play
video.<br>

22.<br>
what kind of **challenges** we are facing today and how we can actually **overcome** them and how are the **models** and in all those big players in the **market**
are trying to overcome those.<br>
One is obviously the **legal and compliance**, **copyright infringement**, **data privacy**, right, because we've been using quite a lot of **data**.<br>
To train those models, what data will be used by those models to **train** themselves.<br> You know what kind of **customer information** or kind of **data**
will be used to train those models.<br> So but a lot of those **big companies** like **Entropic and they're promising** that we that they don't use their **data**.<br>
And also if the companies are actually, you know, worried about their **data** and **data privacy**, obviously they can use the **open source model**, which they
can actually **host it on their environment** so that the **data doesn't leave their premises**.<br> So there are a lot of other mitigation.<br> **Strategies we can**
**implement to overcome this problem,**
**but the bigger one is hallucination** because these models are so **smarter** sometimes when they they **generate response it at most feels like it's a real**

implement to overcome this problem,
- but the bigger one is **hallucination** because these models are so smarter sometimes when they they generate response it at most feels like it's a **real
response**, but that might be incorrect.<br> So how we can actually go through the go through this problem and solve or mitigate this problem? I think
there's.<br>
Things which I explained you to the reg, right? So I always ask model to refer to your own enterprise knowledge to reduce that **hallucination** because
it always rely on the **factual data** rather than the **data it's trained on**.<br>
- Also **ethical concerns** where we are actually when the models are creating responses, whether it's using some **biased**.<br>
And **discrimination language** misinformation or job displacement, right.<br> So when we're actually building those AI solutions, obviously we are not
replacing the humans, we are **complementing the humans**, right? So we're trying to automate only those **monotonous tasks**.<br> So that the employees of
the **workforce** can focus on **more**.<br>
I would say the more **impactful work** or more **creative work** rather than getting bogged down into those overhead.<br> So I think this is how we actually
present the the use of AI in into the organization and how we put that **narrative in place**.<br>
- I think that's really important and also the **explainability** because these models are trained across **large amount of data**, large corpus of the
different **discrete set of data**, right? And when it makes the decision, how do we know whether it's making that decision? How actually it arrives at
that decision? So when we actually go through the **auditing of the system**?
For example, it's really quite prominent in in **recruitment industry**, right when we're actually using some AI tools to to scrutinise the the CVS.<br> So
there might be some **discrimination or biases in place into the system**.<br> So how does an AI system when it scrutinises those **CVS**
arrives at the decision it needs to have the the clear line of of **visibility** in terms of the the in terms of the steps it takes to arrive at that
decision, right.<br> Whether it's it's be accepted or declined, something like that.<br> So we need to build, we need to have that **explainability built into the
solutions** which we build and and a lot of those.<br>
You know the platforms and applications dose provide all those capabilities like **model evaluation**, **LL images**, there's a lot of things already baked
into those big platforms, right? To overcome those issues.<br> So while there is an **exciting word** out there and we can actually start implementing.<br>
Different parts of the business, but we need to be **cognizant about these challenges** and make sure that we address those challenges with the right
solution in place, right, that's it.<br> There are some **exciting sessions coming in today and tomorrow** as well.<br> Please do plug in.<br> There's some of the
things which we talked about.<br> We'll dive deep into.<br>
Some of those things like **Agent TKI, prompt engineering and all this stuff**, so do tune into those sessions and make most out of it and also put some
useful links here.<br> Not many because I guess a lot of things available you can ask our **GPT right?** So.<br>
But for **port engineering agent take AII** feel there's some of those important things, like how **retail media is changing using AI**.<br> I hope you enjoyed this session.<br>